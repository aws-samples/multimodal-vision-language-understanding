# multimodal models for vision-language understanding

Since the release of Claude3, multimodal functionality has increasingly become a promising solution across various business scenarios. However, as fine-tuning Claude3 is not an option, open-source models like [LLaVA-next-Mistral7B](https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b) serve as alternatives for fine-tuning multimodal models. The instruction-following dataset is crucial for training or fine-tuning multimodal models to achieve the desired performance. In this project, we provide a comprehensive guide and source code for generating instruction-following data, as well as for training and deploying the [LLaVA-next-Mistral7B](https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b) model on SageMaker. Additionally, we include use cases such as defect detection and video search to demonstrate the practical applications of multimodal models.

## Instruction-following data generation with Mixtral-8x7B
Since the instruction-following data generated by GPT-4, such as the [LLaVA Visual Instruct 150K Dataset](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K), is restricted for commercial use, we propose generating instruction-following data [using Mixtral-8x7B with SageMaker Jumpstart](https://aws.amazon.com/blogs/machine-learning/mixtral-8x7b-is-now-available-in-amazon-sagemaker-jumpstart/). This approach will allow us to train a model on a dataset that can be used for commercial purposes.

## Train LLaVA-next-Mistral7B on SageMaker

Comming soon

## Deploy LLaVA-next-Mistral7B on SageMaker

**Update on Large Model Inference 0.29**  
- [vLLM Engine User Guide](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/vllm_user_guide.html#vllm-engine-user-guide) 
- [Chat Completions API Schema](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/chat_input_output_schema.html)

Multi Modal Models native support for 

- LlaVA-NeXT (llava-hf/llava-v1.6-mistral-7b-hf, llava-hf/llava-v1.6-vicuna-7b-hf, etc.)
- Phi-3-Vision (microsoft/Phi-3-vision-128k-instruct, etc.)



In [Llava-v16-Mistral-7B-on-SageMaker](Llava-v16-Mistral-7B-on-SageMaker), we deploy [LLaVA-next-Mistral7B](https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b) on a SageMaker endpoint using DJL and an LMI container image. In our sample, we deploy the pretrained `LLaVA-next-Mistral7B`. You can deploy your own LLaVA model if you train your LLaVA model on your own custom dataset.

## Use case demo

Comming soon

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the MIT-0 License. See the LICENSE file.

