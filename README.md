# multimodal models for vision-language understanding

Since the release of Claude3, multimodal functionality has increasingly become a promising solution across various business scenarios. However, as fine-tuning Claude3 is not an option, open-source models like [LLaVA-next-Mistral7B](https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b) serve as alternatives for fine-tuning multimodal models. The instruction-following dataset is crucial for training or fine-tuning multimodal models to achieve the desired performance. In this project, we provide a comprehensive guide and source code for generating instruction-following data, as well as for training and deploying the [LLaVA-next-Mistral7B](https://huggingface.co/liuhaotian/llava-v1.6-mistral-7b) model on SageMaker. Additionally, we include use cases such as defect detection and video search to demonstrate the practical applications of multimodal models.

## Instruction-following data generation with Mixtral-8x7B
Since the instruction-following data generated by GPT-4, such as the [LLaVA Visual Instruct 150K Dataset](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K), is restricted for commercial use, we propose generating instruction-following data [using Mixtral-8x7B with SageMaker Jumpstart](https://aws.amazon.com/blogs/machine-learning/mixtral-8x7b-is-now-available-in-amazon-sagemaker-jumpstart/). This approach will allow us to train a model on a dataset that can be used for commercial purposes.

## Train LLaVA-next-Mistral7B on SageMaker

Comming soon

## Deploy LLaVA-next-Mistral7B on SageMaker

Comming soon

## Use case demo

Comming soon

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the MIT-0 License. See the LICENSE file.

